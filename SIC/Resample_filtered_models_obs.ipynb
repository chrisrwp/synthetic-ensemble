{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e68f93",
   "metadata": {},
   "source": [
    "# Resample detrended filtered and different time period linear detrended modeled and observed SIC\n",
    "\n",
    "### Author: Chris Wyburn-Powell, [github](https://github.com/chrisrwp/synthetic-ensemble/SIC/Resample_filtered_models_obs.ipynb)\n",
    "\n",
    "**Input**: <br>\n",
    "- Observations: Detrended relative to a 2 year lowpass Buttworth filter\n",
    "- Detrended CLIVAR LE Archive model output from CanESM2, CESM1, CSIRO MK3.6, GDL CM3, GFDL ESM2M, MPI ESM1. Detrended relative to a 2 year lowpass Butterworth filter.\n",
    "\n",
    "**Output**: <br>\n",
    "- Mean and standard deviation of 1000 resamplings. Each resampling is itself the standard deviation (with respect to time) of SIC with a 2 year block bootstrap size. Separate files for each model and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d6eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:27 UTC Fri 2022-04-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import dask\n",
    "\n",
    "print(datetime.datetime.utcnow().strftime(\"%H:%M UTC %a %Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5921764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34518 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.12.206.47:37903</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.hpc.ucar.edu/stable/user/cwpowell/proxy/{port}/status' target='_blank'>https://jupyterhub.hpc.ucar.edu/stable/user/cwpowell/proxy/{port}/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>14</li>\n",
       "  <li><b>Cores: </b>14</li>\n",
       "  <li><b>Memory: </b>14.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.12.206.47:37903' processes=14 threads=14, memory=14.00 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for running on Cheyenne, takes ~2.5 minutes per month for models, ~5 seconds for HadISST1 per month, ~50 seconds for NSIDC datasets per month\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = PBSCluster(cores    = 1,\n",
    "                     memory   = '1GB',\n",
    "                     queue    = 'casper', #'economy' for Cheyenne, 'casper' for Casper\n",
    "                     walltime = '00:10:00')\n",
    "\n",
    "cluster.scale(16)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e6f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/glade/scratch/cwpowell/Synthetic_ensemble/'\n",
    "\n",
    "model_names  = ['CanESM2', 'CESM1', 'CSIRO_MK36', 'GFDL_CM3', 'GFDL_ESM2M', 'MPI_ESM1' ]\n",
    "mem_len      = [50,        40,      30,           20,         30,           100        ]\n",
    "\n",
    "lat_labs = ['lat', 'j', 'lat', 'rlat', 'lat', 'j']\n",
    "lon_labs = ['lon', 'i', 'lon', 'rlon', 'lon', 'i']\n",
    "\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "               'August', 'September', 'October', 'November', 'December']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77df5f",
   "metadata": {},
   "source": [
    "# Define the resampling function with a 2 year block bootstrap size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d113d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_boot2_mem(data, time_period, resamp_n, lat_lab, lon_lab):\n",
    "    '''\n",
    "    Resample a 3D time series using a 2 year block boostrap size with replacement\n",
    "    3D so can only resample one member at a time\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: xarray Dataarray,\n",
    "        Detredned SIC dataset with lat, lon and time (single member)\n",
    "    time_period : integer,\n",
    "        For 1979-2020 use 42 as the total number of years in that time period\n",
    "    resamp_n : integer,\n",
    "        The number of times for resampling to take place\n",
    "    lat_lab: string,\n",
    "        Name of coordinate for latitude, e.g. 'lat', 'rlat', 'j'\n",
    "    lon_lab: string,\n",
    "        Name of coordinate for longitude, e.g. 'lon', 'rlon', 'i'\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        Two 2D xarray dataarray objects of resamplings of the input data with standard deviation with respect to time already computed, shape: (lat, lon)\n",
    "    '''  \n",
    "    \n",
    "    #create an xarray dataarray of indexes for half the length of the time period, year_i coordinates 1,3,5...\n",
    "    boot_2_first_ind = xr.DataArray(data   = np.random.randint(0,time_period-2, (resamp_n, int(time_period/2))), \n",
    "                                    coords = {'resampling':np.arange(1,resamp_n+1,1), 'year_i':np.arange(1,time_period+1,2)},\n",
    "                                    dims   = ['resampling', 'year_i'])\n",
    "\n",
    "    #create an identical dataarray but with each element incremented by 1, year_i coordinates 2,4,6....\n",
    "    boot_2_second_ind = (boot_2_first_ind+1).copy()\n",
    "    boot_2_second_ind['year_i'] = np.arange(2,time_period+2,2) #make the coordinates incremented by 1 as well\n",
    "\n",
    "    #concatenate the two arrays with the year_i coordinates in order, this allows a 2 year block boostrap size\n",
    "    all_boot_2_ind = xr.concat((boot_2_first_ind, boot_2_second_ind), dim='year_i').sortby('year_i')\n",
    "    \n",
    "    #initialize a numpy array for all the resampled data after standard deviations have been applied\n",
    "    resampled = np.empty((resamp_n, len(data[lat_lab]), len(data[lon_lab])))\n",
    "    for resamp_i in range(resamp_n): #loop through all of the resamplings\n",
    "        resampled[resamp_i] = data.isel(time=all_boot_2_ind.isel(resampling=resamp_i)).std('year_i')\n",
    "    \n",
    "    #convert the numpy array into an xarray\n",
    "    resampled_xr = xr.DataArray(data   = resampled, \n",
    "                                coords = {'resampling':np.arange(1,resamp_n+1), lat_lab:data[lat_lab], lon_lab:data[lon_lab]}, \n",
    "                                dims   = ['resampling', lat_lab, lon_lab])    \n",
    "    \n",
    "    #compute the final product of the mean and standard deviations across resamplings\n",
    "    mean_resampled = resampled_xr.mean('resampling', skipna=True)\n",
    "    SD_resampled   = resampled_xr.std('resampling', skipna=True)\n",
    "\n",
    "    return(mean_resampled, SD_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8368c7",
   "metadata": {},
   "source": [
    "# Resample models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18169a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 18:39:41.717095 1\n",
      "2022-04-14 18:42:22.056827 2\n",
      "2022-04-14 18:44:27.531821 3\n",
      "2022-04-14 18:46:32.424045 4\n",
      "2022-04-14 18:48:37.786059 5\n",
      "2022-04-14 18:50:43.054911 6\n",
      "2022-04-14 18:52:47.508708 7\n",
      "2022-04-14 18:54:51.405994 8\n",
      "2022-04-14 18:56:54.978755 9\n",
      "2022-04-14 18:58:59.360294 10\n"
     ]
    }
   ],
   "source": [
    "start_yr = 1979\n",
    "end_yr = 2010\n",
    "\n",
    "for month_ in np.arange(1,13):\n",
    "    print(datetime.datetime.now(), month_)\n",
    "    for model_i, model_name in enumerate(model_names):\n",
    "\n",
    "        #load the correct detrended data file\n",
    "        # detrended = xr.open_dataarray('/glade/scratch/cwpowell/Synthetic_ensemble_revisions/SIC/Detrend_filter/'+\n",
    "        #                               '{}_detrended_2yr_filter_1979_2020_{}.nc'.format(model_name, str(month_).zfill(2)))  \n",
    "        detrended = xr.open_dataarray('/glade/scratch/cwpowell/Synthetic_ensemble_revisions/SIC/Detrended/'+\n",
    "                                      '{}_detrended_{}_individual_{}_{}.nc'.format(model_name, \n",
    "                                          str(month_).zfill(2), start_yr, end_yr,))\n",
    "        \n",
    "        ###############################################################################\n",
    "        # compute resampling of all members at once using dask\n",
    "        all_resamp = [] #initialize a list to store the delayed objects\n",
    "        for mem_i, mem in enumerate(np.arange(1,mem_len[model_i]+1)): #loop through all members\n",
    "            #append delayed objects for computation simulatneously later on\n",
    "            all_resamp.append(dask.delayed(resample_boot2_mem)(detrended.sel(member=mem), end_yr-start_yr+1,\n",
    "                                                               1000, lat_labs[model_i], lon_labs[model_i]))\n",
    "\n",
    "        results_resamp = dask.compute(*all_resamp) #do the simultaneous computation on all members \n",
    "\n",
    "        ###############################################################################\n",
    "        # convert the xarray object tuples into an xarray dataset with correct coordinates\n",
    "        all_means, all_SDs = [], []\n",
    "        for mem_i in range(mem_len[model_i]):\n",
    "            all_means.append(results_resamp[mem_i][0])\n",
    "            all_SDs.append(results_resamp[mem_i][1])\n",
    "\n",
    "        save_resamp = xr.Dataset({'mean':xr.concat((all_means), dim='member'), 'SD':xr.concat((all_SDs), dim='member')})\n",
    "        save_resamp['member'] = np.arange(1,mem_len[model_i]+1) #label the member dimension\n",
    "\n",
    "        ###############################################################################\n",
    "        # save the xarray dataset to NetCDF\n",
    "        save_resamp.attrs = {'Description': 'Resampled standard deviations with respect to time of Arctic sea ice concentrations (SIC) for model {}. '\\\n",
    "                        +'Mean - mean standard deviation across the 1000 resamplings, SD - standard deviation across the 1000 resamplings. '\\\n",
    "                        +'Years 1979-2020 for the month of {} are resampled 1000 times with a 2 year bootstrap size. '\\\n",
    "                        # +'SIC anomalies were calculated from a 2 year lowpass Butterworth filter on the individual member and '\\\n",
    "                        +'SIC anomalies were calculated from linear detrending on the individual member at '\\\n",
    "                        +'each grid cell.'.format(model_name, month_names[month_-1]),\n",
    "                             'Units'      : '%',\n",
    "                             'Timestamp'  : str(datetime.datetime.utcnow().strftime(\"%H:%M UTC %a %Y-%m-%d\")),                \n",
    "                             'Data source': 'CLIVAR Large Ensemble Archive (doi: 10.1038/s41558-020-0731-2)',\n",
    "                             'Analysis'   : 'Python 3.7.9 - https://github.com/chrisrwp/synthetic-ensemble/SIC/Resample_filtered_models_obs.ipynb',\n",
    "                        }\n",
    "\n",
    "        save_resamp.to_netcdf('/glade/scratch/cwpowell/Synthetic_ensemble_revisions/SIC/Resampled'+\n",
    "                              # '_filter/{}_resampled_2yr_filter_1979_2020_{}.nc'.format(model_name, str(month_).zfill(2)))\n",
    "                              '/{}_resampled_individual_{}_{}_{}.nc'.format(model_name, start_yr, end_yr, str(month_).zfill(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3643f3d-d70a-4dfa-9067-e9dda4b885a0",
   "metadata": {},
   "source": [
    "# Resample Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32196ce6-82e4-4f80-af56-3a05c12cea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_yr = 1989\n",
    "end_yr = 2020\n",
    "\n",
    "for dataset_name in ['NSIDC_NT', 'NSIDC_BT', 'HadISST1']: #'NSIDC_CDR', \n",
    "    print(datetime.datetime.now(), dataset_name)\n",
    "    \n",
    "    if dataset_name == 'HadISST1':\n",
    "        lat_lab = 'latitude'\n",
    "        lon_lab = 'longitude'\n",
    "        data_doi = 'doi:10.1029/2002JD002670'\n",
    "    else:\n",
    "        lat_lab = 'ygrid'\n",
    "        lon_lab = 'xgrid'\n",
    "        data_doi = 'doi:10.7265/efmz-2t65'\n",
    "    \n",
    "    #load the correct detrended data file\n",
    "    detrended = xr.open_dataarray('/glade/scratch/cwpowell/Synthetic_ensemble_revisions/SIC/'\\\n",
    "                                  # +'Detrend_filter/{}_SIC_2yr_filter_1979_2020.nc'.format(dataset_name))  \n",
    "                                  +'Detrended/{}_detrended_individual_{}_{}.nc'.format(dataset_name, start_yr, end_yr))  \n",
    "            \n",
    "#     ############################## WITH DASK ###############################\n",
    "#     all_resamp = []\n",
    "#     for month_ in np.arange(1,13):\n",
    "\n",
    "#         month_data = detrended.sel(time=detrended['time.month']==month_)\n",
    "\n",
    "#         ###############################################################################\n",
    "#         # compute resampling of each detrending technique\n",
    "#         with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "#             all_resamp.append(dask.delayed(resample_boot2_mem)(month_data, end_yr-start_yr+1, \n",
    "#                                                                1000, lat_lab, lon_lab))\n",
    "#     results_resamp = dask.compute(*all_resamp)\n",
    "    \n",
    "#     ###############################################################################\n",
    "#     # convert the xarray object tuples into an xarray dataset with correct coordinates\n",
    "#     all_means, all_SDs = [], []\n",
    "#     for month_ in np.arange(1,13):\n",
    "#         all_means.append(results_resamp[month_-1][0])\n",
    "#         all_SDs.append(results_resamp[month_-1][1])\n",
    "#     ############################## WITH DASK ###################################\n",
    "    \n",
    "    ############################## WITHOUT DASK ###################################\n",
    "    all_means = [] \n",
    "    all_SDs = [] \n",
    "    \n",
    "    for month_ in np.arange(1,13):\n",
    "        print(datetime.datetime.now(), month_)\n",
    "        \n",
    "        month_data = detrended.sel(time=detrended['time.month']==month_)\n",
    "        \n",
    "        temp_data = resample_boot2_mem(month_data, end_yr-start_yr+1, 1000, lat_lab, lon_lab)\n",
    "        all_means.append(temp_data[0])\n",
    "        all_SDs.append(temp_data[1])\n",
    "    \n",
    "    ############################## WITHOUT DASK ###################################\n",
    "                              \n",
    "    save_resamp = xr.Dataset({'mean':xr.concat((all_means), dim='month'), 'SD':xr.concat((all_SDs), dim='month')})\n",
    "    save_resamp['month'] = np.arange(1,13)\n",
    "\n",
    "    ###############################################################################\n",
    "    # save the xarray dataset to NetCDF\n",
    "    save_resamp.attrs = {'Description': 'Resampled standard deviations with respect to time of Arctic sea ice concentrations '\\\n",
    "                         +'(SIC) for the observational dataset {}. Mean - mean standard deviation across the 1000 resamplings, '\\\n",
    "                         +'SD - standard deviation across the 1000 resamplings. All months for the years {}-{} are '\\\n",
    "                         +'resampled 1000 times with a 2 year bootstrap size. SIC anomalies were calculated from '\\\n",
    "                         # +'a 2 year lowpass Butterworth filter '\\\n",
    "                         +'an ordinary least squares regression linear trend '\\\n",
    "                         +'on each individual grid cell.'.format(dataset_name, start_yr, end_yr),\n",
    "                         'Units'      : '%',\n",
    "                         'Timestamp'  : str(datetime.datetime.utcnow().strftime(\"%H:%M UTC %a %Y-%m-%d\")),                \n",
    "                         'Data source': data_doi,\n",
    "                         'Analysis'   : 'Python 3.7.9 - https://github.com/chrisrwp/synthetic-ensemble/SIC/Resample_filtered_models_obs.ipynb'}\n",
    "\n",
    "    save_resamp.to_netcdf('/glade/scratch/cwpowell/Synthetic_ensemble_revisions/SIC/'\\\n",
    "                          # +'Resampled_filter/{}_resampled_SIC_2yr_filter_1979_2020.nc'.format(dataset_name))\n",
    "                          +'Resampled/{}_resampled_SIC_individual_{}_{}.nc'.format(dataset_name, start_yr, end_yr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL-3.7.9",
   "language": "python",
   "name": "npl-3.7.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
