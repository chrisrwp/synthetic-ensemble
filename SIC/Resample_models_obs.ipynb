{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e68f93",
   "metadata": {},
   "source": [
    "# Resample detrended model and observational SIC\n",
    "\n",
    "### Author: Chris Wyburn-Powell, [github](https://github.com/chrisrwp/blob/main/synthetic-ensemble/SIC/Resample_models_obs.ipynb)\n",
    "\n",
    "**Input**: <br>\n",
    "- Observations: Detrended relative to a linear trend and jackknife for HadISST1 and NSIDC CDR\n",
    "- Detrended CLIVAR LE Archive model output from CanESM2, CESM1, CSIRO MK3.6, GDL CM3, GFDL ESM2M, MPI ESM1. Detrended in the with respect to the following metrics:\n",
    "  * Ensemble mean, i.e. the linear trend of the mean of all members\n",
    "  * Ensemble mean with adjustments so the ensemble mean trend does not reach below 0% or above 100% SIC\n",
    "  * Individual mean, i.e. the linear trend of the member which is being detrended \n",
    "  * Individual mean with adjustments to within 0-100% SIC\n",
    "  * 2 year lowpass Butterworth filter\n",
    "\n",
    "**Output**: <br>\n",
    "- Mean and standard deviation of 1000 resamplings. Each resampling is itself the standard deviation (with respect to time) of SIC with a 2 year block bootstrap size. Separate files for each model, month, and type of detrending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d6eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:47 UTC Fri 2021-08-20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import dask\n",
    "\n",
    "print(datetime.datetime.utcnow().strftime(\"%H:%M UTC %a %Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5921764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for running on Cheyenne - 30 min is sufficeint to run 2 months for all 4 adj/not adj, ens/ind combinations\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = PBSCluster(cores    = 1,\n",
    "                     memory   = '5GB',\n",
    "                     queue    = 'economy',\n",
    "                     walltime = '01:59:00')\n",
    "\n",
    "cluster.scale(16)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e6f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/glade/scratch/cwpowell/Synthetic_ensemble/'\n",
    "\n",
    "model_names  = ['CanESM2', 'CESM1', 'CSIRO_MK36', 'GFDL_CM3', 'GFDL_ESM2M', \n",
    "                'MPI_ESM1' ]\n",
    "mem_len      = [50, 40, 30, 20, 30, 100]\n",
    "\n",
    "lat_labs = ['lat', 'j', 'lat', 'rlat', 'lat', 'j']\n",
    "lon_labs = ['lon', 'i', 'lon', 'rlon', 'lon', 'i']\n",
    "\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "               'August', 'September', 'October', 'November', 'December']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77df5f",
   "metadata": {},
   "source": [
    "# Define the resampling function with a 2 year block bootstrap size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d113d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_boot2_mem(data, time_period, resamp_n, lat_lab, lon_lab):\n",
    "    '''\n",
    "    Resample a 3D time series using a 2 year block boostrap size with \n",
    "    replacement 3D so can only resample one member at a time\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: xarray Dataarray,\n",
    "        Detredned SIC dataset with lat, lon and time (single member)\n",
    "    time_period : integer,\n",
    "        For 1979-2020 use 42 as the total number of years in that time period\n",
    "    resamp_n : integer,\n",
    "        The number of times for resampling to take place\n",
    "    lat_lab: string,\n",
    "        Name of coordinate for latitude, e.g. 'lat', 'rlat', 'j'\n",
    "    lon_lab: string,\n",
    "        Name of coordinate for longitude, e.g. 'lon', 'rlon', 'i'\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        Two 2D xarray dataarray objects of resamplings of the input data with \n",
    "        standard deviation with respect to time already computed, \n",
    "        shape: (lat, lon)\n",
    "    '''  \n",
    "    \n",
    "    #create an xarray dataarray of indexes for half the length of the \n",
    "    #time period, year_i coordinates 1,3,5...\n",
    "    boot_2_first_ind = xr.DataArray(\n",
    "        data   = np.random.randint(0,time_period-2, \n",
    "                                   (resamp_n, int(time_period/2))), \n",
    "        coords = {'resampling':np.arange(1,resamp_n+1,1), \n",
    "                  'year_i':np.arange(1,time_period+1,2)},\n",
    "        dims   = ['resampling', 'year_i']\n",
    "    )\n",
    "\n",
    "    #create an identical dataarray but with each element incremented by 1, \n",
    "    #year_i coordinates 2,4,6....\n",
    "    boot_2_second_ind = (boot_2_first_ind+1).copy()\n",
    "    #make the coordinates incremented by 1 as well\n",
    "    boot_2_second_ind['year_i'] = np.arange(2,time_period+2,2) \n",
    "\n",
    "    #concatenate the two arrays with the year_i coordinates in order,\n",
    "    #this allows a 2 year block boostrap size\n",
    "    all_boot_2_ind = xr.concat((boot_2_first_ind, boot_2_second_ind), \n",
    "                               dim='year_i').sortby('year_i')\n",
    "    \n",
    "    #initialize a numpy array for all the resampled data after \n",
    "    #standard deviations have been applied\n",
    "    resampled = np.empty((resamp_n, len(data[lat_lab]), len(data[lon_lab])))\n",
    "    for resamp_i in range(resamp_n): #loop through all of the resamplings\n",
    "        resampled[resamp_i] = data.isel(time=all_boot_2_ind.isel(\n",
    "                                   resampling=resamp_i)).std('year_i')\n",
    "    \n",
    "    #convert the numpy array into an xarray\n",
    "    resampled_xr = xr.DataArray(\n",
    "        data   = resampled, \n",
    "        coords = {'resampling':np.arange(1,resamp_n+1), \n",
    "                  lat_lab:data[lat_lab], \n",
    "                  lon_lab:data[lon_lab]}, \n",
    "        dims   = ['resampling', lat_lab, lon_lab]\n",
    "    )    \n",
    "    \n",
    "    #compute mean and standard deviations across resamplings\n",
    "    mean_resampled = resampled_xr.mean('resampling', skipna=True)\n",
    "    SD_resampled   = resampled_xr.std('resampling', skipna=True)\n",
    "\n",
    "    return(mean_resampled, SD_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8368c7",
   "metadata": {},
   "source": [
    "# Resample models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18169a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for adj in ['', 'adj_']:\n",
    "    print(datetime.datetime.now(), adj)\n",
    "    for ind_ens in ['ensemble', 'individual']:\n",
    "        print(datetime.datetime.now(), ind_ens)\n",
    "        for month_ in np.arange(1,13):\n",
    "            print(datetime.datetime.now(), month_)\n",
    "            for model_i, model_name in enumerate(model_names):\n",
    "\n",
    "                #load the correct detrended data file\n",
    "                detrended = xr.open_dataset(data_path+'SIC/Detrended/{}_'\\\n",
    "                                            +'detrended_{}{}_{}_1979_2020'\\\n",
    "                                            +'.nc'.format(model_name, adj, \n",
    "                                            str(month_).zfill(2), ind_ens))  \n",
    "\n",
    "                ################################################################\n",
    "                # compute resampling of all members at once using dask\n",
    "                all_resamp = [] #initialize a list to store the delayed objects\n",
    "                \n",
    "                #loop through all members\n",
    "                for mem_i, mem in enumerate(np.arange(1,mem_len[model_i]+1)): \n",
    "                    #append delayed objects for computation simulatneously later\n",
    "                    all_resamp.append(dask.delayed(resample_boot2_mem)\n",
    "                                      (detrended.sel(member=mem)['SIC'], 42, \n",
    "                                       1000, lat_labs[model_i], \n",
    "                                       lon_labs[model_i]))\n",
    "\n",
    "                results_resamp = dask.compute(*all_resamp) #do the simultaneous computation on all members \n",
    "\n",
    "                ################################################################\n",
    "                #add correct coords to xarray object tuples\n",
    "                all_means, all_SDs = [], []\n",
    "                for mem_i in range(mem_len[model_i]):\n",
    "                    all_means.append(results_resamp[mem_i][0])\n",
    "                    all_SDs.append(results_resamp[mem_i][1])\n",
    "\n",
    "                save_resamp = xr.Dataset(\n",
    "                    {'mean':xr.concat((all_means), dim='member'), \n",
    "                     'SD':xr.concat((all_SDs), dim='member')}\n",
    "                )\n",
    "                \n",
    "                #label the member dimension\n",
    "                save_resamp['member'] = np.arange(1,mem_len[model_i]+1) \n",
    "\n",
    "                ################################################################\n",
    "                # save the xarray dataset to NetCDF\n",
    "                save_resamp.attrs = {\n",
    "                    'Description': 'Resampled standard deviations with respect'\\\n",
    "                        +' to time of Arctic sea ice concentrations (SIC) for '\\\n",
    "                        +'model {}. Mean - mean standard deviation across the '\\\n",
    "                        +'1000 resamplings, SD - standard deviation across the'\\\n",
    "                        +' 1000 resamplings. Years 1979-2020 for the month of '\\\n",
    "                        +'{} are resampled 1000 times with a 2 year bootstrap '\\\n",
    "                        +'size. SIC anomalies were calculated from a linear '\\\n",
    "                        +'trend of the {} members'.format(model_name, \n",
    "                        month_names[month_-1], ind_ens),\n",
    "                    'Units'      : '%',\n",
    "                    'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "                        \"%H:%M UTC %a %Y-%m-%d\")),                \n",
    "                    'Data source': 'CLIVAR Large Ensemble Archive '\\\n",
    "                        +'(doi: 10.1038/s41558-020-0731-2)',\n",
    "                    'Analysis'   : 'Python 3.7.9 - https://github.com/chrisrwp'\\\n",
    "                        +'/synthetic-ensemble/SIC/Resample_models_obs.ipynb'\n",
    "                }\n",
    "\n",
    "                save_resamp.to_netcdf(\n",
    "                    data_path+'SIC/Resampled/{}_resampled_{}_{}{}.nc'.format(\n",
    "                    model_name, str(month_).zfill(2), adj, ind_ens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0ba50-f0a2-495f-9611-bd2f45344028",
   "metadata": {},
   "source": [
    "# Resample Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32196ce6-82e4-4f80-af56-3a05c12cea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just do March and September for now to save time\n",
    "month_list = [3,9]\n",
    "\n",
    "for dataset_name in ['HadISST1', 'Merged_Hadley_OI', 'NSIDC_BT', 'NSIDC_CDR', \n",
    "                     'NSIDC_NT']:\n",
    "    print(dataset_name)\n",
    "    if dataset_name in ['HadISST1', 'Merged_Hadley_OI']:\n",
    "        lat_lab = 'latitude'\n",
    "        lon_lab = 'longitude'\n",
    "    else:\n",
    "        lat_lab = 'ygrid'\n",
    "        lon_lab = 'xgrid'\n",
    "    \n",
    "    for detrend_type in ['individual', 'adj_individual', 'jackknife', \n",
    "                         'adj_jackknife']:\n",
    "        print(datetime.datetime.now(), detrend_type)\n",
    "            \n",
    "        #load the correct detrended data file\n",
    "        if dataset_name in ['HadISST1', 'Merged_Hadley_OI']:\n",
    "            detrended = xr.open_dataset(\n",
    "                data_path+'SIC/Detrended/{}_detrended_{}_1979_2020.nc'.format(\n",
    "                dataset_name, detrend_type), chunks=100)  \n",
    "        else:\n",
    "            detrended = xr.open_dataset(\n",
    "                data_path+'SIC/Detrended/{}_detrended_{}_1979_2020_03_09'\\\n",
    "                +'.nc'.format(dataset_name, detrend_type), chunks=100)\n",
    "        \n",
    "        attrs_dict = detrended.attrs #get the attributes from the netcdf file\n",
    "        \n",
    "        mean_resamp = []\n",
    "        SD_resamp   = []\n",
    "        for month_ in month_list:\n",
    "            print(datetime.datetime.now(), month_)\n",
    "            \n",
    "            if detrend_type in ['jackknife', 'adj_jackknife']:\n",
    "                month_data = detrended['SIC'].sel(\n",
    "                    time=detrended['time.month']==month_).mean(\n",
    "                    'jackknife', skipna=True).compute()\n",
    "            else:\n",
    "                month_data = detrended['SIC'].sel(\n",
    "                    time=detrended['time.month']==month_).compute()\n",
    "            \n",
    "            ####################################################################\n",
    "            #compute resampling of each detrending technique\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "                temp_resamp = resample_boot2_mem(month_data, 42, 1000, \n",
    "                                                 lat_lab, lon_lab)\n",
    "                \n",
    "            mean_resamp.append(temp_resamp[0])\n",
    "            SD_resamp.append(temp_resamp[1])\n",
    "           \n",
    "        save_resamp = xr.Dataset({'mean':xr.concat((mean_resamp), dim='month'),\n",
    "                                  'SD':xr.concat((SD_resamp), dim='month')})\n",
    "        save_resamp['month'] = month_list\n",
    "\n",
    "        ########################################################################\n",
    "        #save the xarray dataset to NetCDF\n",
    "        save_resamp.attrs = {\n",
    "            'Description': 'Resampled detrended standard deviations with '\\\n",
    "                +'respect to time for '+attrs_dict['Description'][10:],\n",
    "            'Units'      : '%',\n",
    "            'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "                \"%H:%M UTC %a %Y-%m-%d\")),                \n",
    "            'Data source': attrs_dict['Data source'],\n",
    "            'Analysis'   : 'Python 3.7.9 - https://github.com/chrisrwp/'\\\n",
    "                +'synthetic-ensemble/SIC/Resample_models_obs.ipynb'}\n",
    "\n",
    "        save_resamp.to_netcdf(data_path+'SIC/Resampled/{}_resampled_{}_1979_20'\\\n",
    "                              +'20_03_09.nc'.format(dataset_name, detrend_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL-3.7.9",
   "language": "python",
   "name": "npl-3.7.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
